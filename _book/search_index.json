[
["probability.html", "Chapter 3 Probability 3.1 Misleading statistical evidence in cot death trials 3.2 Relative frequency definition of probability 3.3 Basic properties of probability 3.4 Conditional probability 3.5 Addition rule of probability 3.6 Multiplication rule of probability 3.7 Law of total probability 3.8 Bayes’ theorem", " Chapter 3 Probability Most people have heard the word probability used in connection with a random experiment, that is, an experiment whose outcome cannot be predicted with certainty, such as tossing a coin, tossing dice, dealing cards etc. We start by considering a criminal case in which fundamental ideas surrounding the use of probability were hugely important. Then we study the concept of probability using the traditional simple example of tossing a coin. 3.1 Misleading statistical evidence in cot death trials In recent years there have been three high-profile criminal cases in which a mother has been put on trial for the murder of a her babies. In each case the medical evidence against the woman was weak and the prosecution relied heavily on statistical arguments to make their case. However, these arguments were not made by a statistician, but by a medical expert witness: Professor Sir Roy Meadows. However, there were two problems with Professor Meadows’ evidence: firstly, it contained serious statistical errors; and secondly, it was presented in a way which is likely to be misinterpreted by a jury. To illustrate the error we consider the case of Sally Clark. Sally Clark’s first child died unexpectedly in 1996 at the age of 3 months. Sally was the only person in the house at the time. There was evidence of a respiratory infection and the death was recorded as natural; a case of Sudden Infant Death Syndrome (SIDS), or cot death. In 1998 Sally’s second child died in similar circumstances at the age of 2 months. Sally was then charged with the murder of both babies. There was some medical evidence to suggest that the second baby could have been smothered, although this could be explained by an attempt at resuscitation. It appeared that the decision to charge Sally was based partly on the reasoning that cot death is quite rare so having two cot deaths in the same family must be very unlikely indeed. This is the basis of Professor Meadows’ assertion that: “One cot death is a tragedy, two cot deaths is suspicious and, until the contrary is proved, three cot deaths is murder.”. At her trial in 1999 Sally Clark was found guilty of murder and sentenced to life imprisonment. Professor Meadows’ statistical evidence At Sally Clark’s trial in 1999 Professor Meadows claimed that, in a family like Sally’s (affluent, non-smoking with a mother aged over 26), the chance of two cot deaths is 1 in 73 million, that is, a probability of 1/73,000,000 \\(\\approx\\) 0.000000014. Professor Meadows had calculated this value based on a study which had estimated the probability of one cot death in a family like Sally’s to be 1 in 8543, that is, 1 cot death occurs for every 8543 of such families. Professor Meadows had then performed the calculation \\[\\frac{1}{8543}\\times\\frac{1}{8543}=\\frac{1}{72,982,849}\\approx\\frac{1}{73,000,000}.\\] There are problems with this evidence, both with this calculation and with the idea that this apparently small number provides evidence of guilt. Can you identify these problems? 3.2 Relative frequency definition of probability Example: tossing a coin If you toss a coin, the outcome (the side on top when the coin falls to the ground) is either a Head (\\(H\\)) or a Tail (\\(T\\)). Suppose that you toss the coin a large number of times. Unless you are very skillful the outcome of each toss depends on chance. Therefore, if you toss the coin repeatedly, the exact sequence of \\(H\\)s and \\(T\\)s is not predictable with certainty in advance. This is usually the case with any experiment. Even if we try very hard to repeat an experiment under exactly the same conditions, there is a certain amount of variability in the results which we cannot explain, but we must accept. The experiment is a random experiment. Nevertheless, if the coin is fair (equally balanced), and it is tossed fairly, we might expect the long run proportion, or relative frequency, of \\(H\\)s to settle down to 1/2. However, the only way to find out whether this is true is to toss a coin repeatedly, forever, and calculate the proportion of tosses on which \\(H\\) is the outcome. It is not possible, in practice, for any experiment to be repeated forever. However, a South African statistician Jon Kerrich managed to toss a coin 10,000 times while imprisioned in Denmark during World War II. At the end of his effort he had recorded 5067 Heads and 4933 Tails. Figure 3.1 shows how the proportion of heads Kerrich threw changed as the number of tosses increased. Figure 3.1: The proportion of heads in a sequence of 10,000 coin tosses. Kerrich (1946). Initially the proportion of heads fluctuates greatly but begins to settle down as the number of tosses increases. After 10,000 tosses the relative frequency of Head is 5067/10,000=0.5067. We might suppose that if Kerrich were able to continue his experiment forever, the proportion of heads would tend to a limiting value which would be very near, if not exactly, 1/2. This hypothetical limiting value if the probability of heads and is denoted by \\(P(H)\\). Looking at this slightly more formally The coin-tossing example motivates the relative frequency or frequentist definition of the probability of an event; namely the relative frequency with which the event occurs in the long run; or, in other words, the proportion of times that the event would occur in an infinite number of identical repeated experiments. Suppose that we toss a coin \\(n\\) times. If the coin is fair, and is tossed fairly, then it is reasonable to suppose that \\[\\mbox{the relative frequency of $H$} \\,\\,=\\,\\, \\frac{\\mbox{number of times $H$ occurs}}{n},\\] tends to 1/2 as \\(n\\) gets larger. We say that the event \\(H\\) has probability 1/2, or \\(P(H)=1/2\\). More generally, consider some event \\(E\\) based on the outcomes of an experiment. Suppose that the experiment can, in principle, be repeated, under exactly the same conditions, forever. Let \\(n(E)\\) denote the number of times that the event \\(E\\) would occur in \\(n\\) experiments. We suppose that \\[\\mbox{the relative frequency of $E$} \\,=\\, \\frac{n(E)}{n} \\,\\longrightarrow\\, P(E)\\,, \\,\\, \\mbox{~as~}n \\longrightarrow \\infty.\\] So, the probability \\(P(E)\\) of the event \\(E\\) is defined as the limiting value of \\(n(E)/n\\) as \\(n \\rightarrow \\infty\\). That is, \\[\\begin{equation} P(E) \\,\\,=\\,\\, \\mathop {\\rm limit}\\limits_{n \\rightarrow \\infty}\\,\\frac{n(E)}{n}, \\tag{3.1} \\end{equation}\\] supposing that this limit exists. (Note: I have written ‘limit’ and not ‘lim’ because this is not a limit in the usual mathematical sense.) In order to satisfy ourselves that the probability of an event exists, we do not need to repeat an experiment an infinite number of times, or even be able to. All we need to do is imagine the experiment being performed repeatedly. An event with probability 0.75, say, would be expected to occur 75 times out of 100 in the long run. An alternative approach, considered in STAT0003, makes a simple set of basic assumptions about probability, called the axioms of probability. Using these axioms it can be proved that the limiting relative frequency in equation (3.1) does exist and that it is equal to \\(P(E)\\). In STAT0002 we will not consider these axioms formally. However, they are so basic and intuitive that you will find that we take them for granted. An aside. There is another definition of probability, the subjective definition, which is the degree of belief someone has in the occurrence of an event, based on their knowledge and any evidence they have already seen. For example, you might reasonably believe that the probability that a coin comes up Heads when it is tossed is 1/2 because the coin looks symmetrical. If you are certain that \\(P(H)=1/2\\) then no amount of evidence from actually tossing the coin will change your mind. However, if you just think that \\(P(H)=1/2\\) is more likely that other values of \\(P(H)\\) then observing many more \\(H\\)s than \\(T\\)s in a long sequence of tosses may lead you to believe that \\(P(H)&gt;1/2\\). We will not consider this definition of probability again in this course. However, it forms the basis of the Bayesian approach to Statistics. You may study this in a more advanced courses, for example, STAT0008 Statistical Inference. Example: tossing a coin (continued) We now look at the coin-tossing example in a slightly different way. If we toss a coin forever we generate an infinite population of outcomes: {H,H,T,H,}, say. Think about choosing, or sampling, one of these outcomes at random from this population. The probability that this outcome is \\(H\\) is the proportion of \\(H\\)s in the population. If we assume that the coin is fair then the infinite population contains 50% \\(H\\)s and 50% \\(T\\)s. In that case \\(P(H)=1/2\\), and \\(P(T)=1/2\\). Example: Graduate Admissioins at Berkeley Table 3.1 contains data relating to graduate admissions in 1973 at the University of California, Berkeley, USA in the six largest (in terms of numbers of admissions) departments of the university. These data are discussed in Bickel, Hammel, and O’Connell (1975). We use the following notation: \\(A\\) for an accepted applicant, \\(R\\) for a rejected applicant. Table 3.2 sumarises the notation used for the frequencies in table 3.1. We will look at this example in more detail later. Table 3.1: Numbers of graduate applicants by outcome. ARtotal 175527714526 Table 3.2: Notation for Table 3.1. ARtotal n(A)n(R)n The population of interest now is the population of graduate applicants to the six largest departments of the Berkeley. If we choose an applicant at random from this population the probability \\(P(A)\\) that they are accepted is given by \\[ P(A) = \\frac{n(A)}{n} = \\frac{1,755}{4,526} = 0.388. \\] Similarly, the probability \\(P(R)\\) that a randomly chosen applicant is rejected is given by \\[ P(R) = \\frac{n(R)}{n} = \\frac{2,771}{4,526} = 0.612. \\] In both the coin-tossing and Berkeley admissions examples we have imagined choosing an individual from a population in such a way that all individuals are equally likely to be chosen. The probability of the individual having a particular property, for example, that an applicant is accepted, is given by the proportion of individuals in the population which have this property. In the coin-tossing example the population is hypothetical, generated by thinking about repeating an experiment an infinite number of times. In the Berkeley admissions example the population is an actual population of people. Notation: sample space, outcomes and events The set of all possible outcomes of a random experiment is called the sample space \\(S\\) of the experiment. We may denote a single outcome of an experiment by \\(s\\). An event \\(E\\) is a collection of outcomes, possible just one outcome. It is very important to define the sample space \\(S\\) carefully. In the coin-tossing example we have \\(S=\\{H,T\\}\\). If the coin is unbiased then the probabilities of the outcomes in \\(S\\) are given by \\(P(H)=1/2\\) and \\(P(T)=1/2\\). 3.3 Basic properties of probability Since we have defined probability as a proportion, basic properties of proportions must also hold for a probability. Consider an event \\(E\\). Then the following must hold \\(0 \\leq P(E) \\leq 1\\); if \\(E\\) is impossible then \\(P(E)=0\\); if \\(E\\) is certain then \\(P(E)=1\\); \\(P(S)=1\\). This is true because the outcome must, by definition, be in the sample space. 3.4 Conditional probability In Section @ref(sids} (the cot death example) the statistical evidence presented to the court was based on the estimate that “the probability of one cot death in a family like Sally Clark’s is 1 in 8543”. What does this mean? The study from which this statistic was taken estimated the overall probability of cot death to be 1 in 1303. That is, cot death occurs in approximately 1 in every 1303 families. However, the study also found that the probability of cot death depended on various characteristics such as, income, smoking status and the age of the mother. For example, the probability of cot death was found to be much greater in families containing one or more smokers than in non-smoking families. The study estimated the probability of cot death for each possible combination of these characteristics. For the combination which is relevant to Sally Clark, whose family was affluent, non-smoking and she was aged over 26, the probability of cot death was estimated to be smaller: 1 in 8543. This (1 in 8543) is a conditional probability. We have conditioned on the event that the family in question is affluent, non-smoking and the mother is aged over 26. The overall probability of cot death (1 in 1303) is often called an unconditional probability. In this example it is perhaps easiest to think of the conditioning as selecting a specific sub-population of families from the complete population of families. Another way to think about this is in terms of the sample space. We have reduced the original sample space - the outcomes (cot death or no cot death) of all families with children - to a subset of this sample space - the outcomes of all affluent, non-smoking families where the mother is over 26. Notation When we are working with conditional probabilities we need to use a neat notation rather than write out long sentences like the ones above. Let \\(C\\) be the event that a family has one cot death. Let \\(F_1\\) be the event that the family in question is affluent, non-smoking, and the mother is over 26. Instead of writing “the probability of one cot death in a family conditional on the type of family is 1 in 8543”\" we write \\[P(C \\mid F_1) = \\frac{1}{8543}.\\] The `\\(\\mid\\)’ sign means “conditional on”’ or, more simply, “given”. Therefore, for \\(P(C \\mid F_1)\\) we might say \"“the probability of event \\(C\\) conditional on event \\(F_1\\)”, or “the probability of event \\(C\\) given event \\(F_1\\)”. The (unconditional) probability of one cot death is given by \\[P(C) = \\frac{1}{1303}.\\] In section 3.1 I did not use the \\(~\\mid~\\) sign in my notation (because we hadn’t seen it then), but I did make the conditioning clear by saying for a family like Sally Clark’s. ] In fact all probabilities are conditional probabilities, because a probability is conditioned on the sample space \\(S\\). When we define \\(S\\) we rule out anything that is not in \\(S\\). So instead of \\(P(C)\\) we could write \\(P(C \\mid S)\\). We do not tend to do this because it takes more time and it tends to make things more difficult to read. However, we should always try to bear in mind the sample space when we think about a probability. We use the following example to illustrate conditional probability, independence and the rules of probability. Table 3.3: Numbers of graduate applicants by sex and outcome. ARtotal M119814932691 F55712781835 total175527714526 Table 3.4: Notation for Table 3.3. ARtotal Mn(M, A)n(M, R)n(M) Fn(F, A)n(F, R)n(F) totaln(A)n(R)n Table 3.5: Proportions based on Table 3.3. ARtotal M0.2650.3300.595 F0.1230.2820.405 total0.3880.6121.000 Table 3.6: Notation for Table 3.5. ARtotal MP(M, A)P(M, R)P(M) FP(F, A)P(F, R)P(F) totalP(A)P(R)1 Table 3.7: Conditioning on applicant being male (frequencies). ARtotal M119814932691 F55712781835 total175527714526 Table 3.8: Conditioning on applicant being male (probabilities). ARtotal M0.2650.3300.595 F0.1230.2820.405 total0.3880.6121.000 Table 3.9: Cells with M or A are shaded (frequencies). ARtotal M119814932691 F55712781835 total175527714526 Table 3.10: Cells with M or A are shaded (probabilities). ARtotal M0.2650.3300.595 F0.1230.2820.405 total0.3880.6121.000 Table 3.11: Distribution of ABO blood groups in the UK. ABO grouppercentage O44 A42 B10 AB4 Table 3.12: Distribution of Rhesus blood groups in the UK. Rhesus grouppercentage Rh+83 Rh-17 Table 3.13: Distribution of Rhesus blood groups in the UK. OABABtotal Rh+0.3490.0830.830 Rh-0.0750.0710.0170.170 total0.4400.4200.1000.0401.000 Table 3.14: Conditioning on applicant being accepted (frequencies). ARtotal M119814932691 F55712781835 total175527714526 Table 3.15: Conditioning on applicant being accepted (probabilites). ARtotal M0.2650.3300.595 F0.1230.2820.405 total0.3880.6121.000 3.5 Addition rule of probability 3.6 Multiplication rule of probability 3.7 Law of total probability 3.8 Bayes’ theorem 3.8.1 An example of independence References "]
]
