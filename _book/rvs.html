<!DOCTYPE html>
<html lang="en-gb" xml:lang="en-gb">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Random variables | STAT0002 Introduction to Probability and Statistics</title>
  <meta name="description" content="Produces STAT0002 notes in an accessible format" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Random variables | STAT0002 Introduction to Probability and Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Produces STAT0002 notes in an accessible format" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Random variables | STAT0002 Introduction to Probability and Statistics" />
  
  <meta name="twitter:description" content="Produces STAT0002 notes in an accessible format" />
  

<meta name="author" content="Paul Northrop, Department of Statistical Science, University College London" />


<meta name="date" content="2020-09-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability.html"/>
<link rel="next" href="simple.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT0002 2020-21</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>The purpose of these notes</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#real"><i class="fa fa-check"></i><b>1.1</b> Real statistical investigations</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#shuttle"><i class="fa fa-check"></i><b>1.2</b> Challenger Space Shuttle Catastrophe</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#uncertainty"><i class="fa fa-check"></i><b>1.2.1</b> Uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#a-very-brief-introduction-to-stochastic-simulation"><i class="fa fa-check"></i><b>1.3</b> A very brief introduction to stochastic simulation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html"><i class="fa fa-check"></i><b>2</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#types-of-data"><i class="fa fa-check"></i><b>2.1</b> Types of data</a><ul>
<li class="chapter" data-level="2.1.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#qualitative-or-categorical-data"><i class="fa fa-check"></i><b>2.1.1</b> Qualitative or categorical data</a></li>
<li class="chapter" data-level="2.1.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#quantitative-or-numerical-data"><i class="fa fa-check"></i><b>2.1.2</b> Quantitative or numerical data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#describing-distributions"><i class="fa fa-check"></i><b>2.2</b> Describing distributions</a><ul>
<li class="chapter" data-level="" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#example-oxford-births-data"><i class="fa fa-check"></i>Example: Oxford births data</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#summary-statistics"><i class="fa fa-check"></i><b>2.3</b> Summary Statistics</a><ul>
<li class="chapter" data-level="2.3.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>2.3.1</b> Five number summary</a></li>
<li class="chapter" data-level="2.3.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#meanstdev"><i class="fa fa-check"></i><b>2.3.2</b> Mean and standard deviation</a></li>
<li class="chapter" data-level="2.3.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#mode"><i class="fa fa-check"></i><b>2.3.3</b> Mode</a></li>
<li class="chapter" data-level="2.3.4" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#symmetry"><i class="fa fa-check"></i><b>2.3.4</b> Symmetry</a></li>
<li class="chapter" data-level="2.3.5" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#correlation"><i class="fa fa-check"></i><b>2.3.5</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#tables"><i class="fa fa-check"></i><b>2.4</b> Tables</a><ul>
<li class="chapter" data-level="2.4.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#frequency-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Frequency distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#graphs"><i class="fa fa-check"></i><b>2.5</b> Graphs (1 variable)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#histogram"><i class="fa fa-check"></i><b>2.5.1</b> Histograms</a></li>
<li class="chapter" data-level="2.5.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#stem"><i class="fa fa-check"></i><b>2.5.2</b> Stem-and-leaf plots</a></li>
<li class="chapter" data-level="2.5.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#dotplots"><i class="fa fa-check"></i><b>2.5.3</b> Dotplots</a></li>
<li class="chapter" data-level="2.5.4" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#boxplots"><i class="fa fa-check"></i><b>2.5.4</b> Boxplots</a></li>
<li class="chapter" data-level="2.5.5" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#barplots"><i class="fa fa-check"></i><b>2.5.5</b> Barplots</a></li>
<li class="chapter" data-level="2.5.6" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#times-series-plots"><i class="fa fa-check"></i><b>2.5.6</b> Times series plots</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#election"><i class="fa fa-check"></i><b>2.6</b> 2000 US Presidential Election</a></li>
<li class="chapter" data-level="2.7" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#graphs2"><i class="fa fa-check"></i><b>2.7</b> Graphs (2 variables)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#scatter-plots"><i class="fa fa-check"></i><b>2.7.1</b> Scatter plots</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#transformation-of-data"><i class="fa fa-check"></i><b>2.8</b> Transformation of data</a><ul>
<li class="chapter" data-level="2.8.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#transsymmetry"><i class="fa fa-check"></i><b>2.8.1</b> Transformation to approximate symmetry</a></li>
<li class="chapter" data-level="2.8.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#straighten"><i class="fa fa-check"></i><b>2.8.2</b> Straightening scatter plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a><ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#sids"><i class="fa fa-check"></i><b>3.1</b> Misleading statistical evidence in cot death trials</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#relative-frequency-definition-of-probability"><i class="fa fa-check"></i><b>3.2</b> Relative frequency definition of probability</a></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#basic-properties-of-probability"><i class="fa fa-check"></i><b>3.3</b> Basic properties of probability</a></li>
<li class="chapter" data-level="3.4" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.4</b> Conditional probability</a></li>
<li class="chapter" data-level="3.5" data-path="probability.html"><a href="probability.html#addition-rule-of-probability"><i class="fa fa-check"></i><b>3.5</b> Addition rule of probability</a><ul>
<li class="chapter" data-level="3.5.1" data-path="probability.html"><a href="probability.html#mutually-exclusive-events"><i class="fa fa-check"></i><b>3.5.1</b> Mutually exclusive events</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="probability.html"><a href="probability.html#multrule"><i class="fa fa-check"></i><b>3.6</b> Multiplication rule of probability</a></li>
<li class="chapter" data-level="3.7" data-path="probability.html"><a href="probability.html#indepevents"><i class="fa fa-check"></i><b>3.7</b> Independence of events</a><ul>
<li class="chapter" data-level="3.7.1" data-path="probability.html"><a href="probability.html#bloodindep"><i class="fa fa-check"></i><b>3.7.1</b> An example of independence</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="probability.html"><a href="probability.html#law-of-total-probability"><i class="fa fa-check"></i><b>3.8</b> Law of total probability</a></li>
<li class="chapter" data-level="3.9" data-path="probability.html"><a href="probability.html#bayes-theorem"><i class="fa fa-check"></i><b>3.9</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="3.10" data-path="probability.html"><a href="probability.html#dna-identification-evidence"><i class="fa fa-check"></i><b>3.10</b> DNA identification evidence</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="rvs.html"><a href="rvs.html"><i class="fa fa-check"></i><b>4</b> Random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="rvs.html"><a href="rvs.html#discrete"><i class="fa fa-check"></i><b>4.1</b> Discrete random variables</a></li>
<li class="chapter" data-level="4.2" data-path="rvs.html"><a href="rvs.html#continuous"><i class="fa fa-check"></i><b>4.2</b> Continuous random variables</a></li>
<li class="chapter" data-level="4.3" data-path="rvs.html"><a href="rvs.html#expectation"><i class="fa fa-check"></i><b>4.3</b> Expectation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="rvs.html"><a href="rvs.html#expectation-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>4.3.1</b> Expectation of a discrete random variable</a></li>
<li class="chapter" data-level="4.3.2" data-path="rvs.html"><a href="rvs.html#expectation-of-a-continuous-random-variable"><i class="fa fa-check"></i><b>4.3.2</b> Expectation of a continuous random variable</a></li>
<li class="chapter" data-level="4.3.3" data-path="rvs.html"><a href="rvs.html#properties-of-mathrmex"><i class="fa fa-check"></i><b>4.3.3</b> Properties of <span class="math inline">\(\mathrm{E}(X)\)</span></a></li>
<li class="chapter" data-level="4.3.4" data-path="rvs.html"><a href="rvs.html#the-expectation-of-gx"><i class="fa fa-check"></i><b>4.3.4</b> The expectation of <span class="math inline">\(g(X)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="rvs.html"><a href="rvs.html#variance"><i class="fa fa-check"></i><b>4.4</b> Variance</a><ul>
<li class="chapter" data-level="4.4.1" data-path="rvs.html"><a href="rvs.html#variance-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> Variance of a discrete random variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="rvs.html"><a href="rvs.html#variance-of-a-continuous-random-variable"><i class="fa fa-check"></i><b>4.4.2</b> Variance of a continuous random variable</a></li>
<li class="chapter" data-level="4.4.3" data-path="rvs.html"><a href="rvs.html#variance-and-standard-deviation"><i class="fa fa-check"></i><b>4.4.3</b> Variance and standard deviation</a></li>
<li class="chapter" data-level="4.4.4" data-path="rvs.html"><a href="rvs.html#properties-of-mathrmvarx"><i class="fa fa-check"></i><b>4.4.4</b> Properties of <span class="math inline">\(\mathrm{var}(X)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rvs.html"><a href="rvs.html#locations"><i class="fa fa-check"></i><b>4.5</b> Other measures of location</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rvs.html"><a href="rvs.html#the-median-of-a-random-variable"><i class="fa fa-check"></i><b>4.5.1</b> The median of a random variable</a></li>
<li class="chapter" data-level="4.5.2" data-path="rvs.html"><a href="rvs.html#the-mode-of-a-random-variable"><i class="fa fa-check"></i><b>4.5.2</b> The mode of a random variable</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rvs.html"><a href="rvs.html#quantiles"><i class="fa fa-check"></i><b>4.6</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> Simple distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="simple.html"><a href="simple.html#the-bernoulli-distribution"><i class="fa fa-check"></i><b>5.1</b> The Bernoulli distribution</a></li>
<li class="chapter" data-level="5.2" data-path="simple.html"><a href="simple.html#binomial"><i class="fa fa-check"></i><b>5.2</b> The binomial distribution</a></li>
<li class="chapter" data-level="5.3" data-path="simple.html"><a href="simple.html#the-geometric-distribution"><i class="fa fa-check"></i><b>5.3</b> The geometric distribution</a></li>
<li class="chapter" data-level="5.4" data-path="simple.html"><a href="simple.html#the-poisson-distribution"><i class="fa fa-check"></i><b>5.4</b> The Poisson distribution</a></li>
<li class="chapter" data-level="5.5" data-path="simple.html"><a href="simple.html#the-uniform-distribution"><i class="fa fa-check"></i><b>5.5</b> The uniform distribution</a></li>
<li class="chapter" data-level="5.6" data-path="simple.html"><a href="simple.html#the-exponential-distribution"><i class="fa fa-check"></i><b>5.6</b> The exponential distribution</a></li>
<li class="chapter" data-level="5.7" data-path="simple.html"><a href="simple.html#normal"><i class="fa fa-check"></i><b>5.7</b> The normal distribution</a></li>
<li class="chapter" data-level="5.8" data-path="simple.html"><a href="simple.html#qq-plots"><i class="fa fa-check"></i><b>5.8</b> QQ plots</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>6</b> Statistical Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="statistical-inference.html"><a href="statistical-inference.html#sample-and-populations"><i class="fa fa-check"></i><b>6.1</b> Sample and populations</a></li>
<li class="chapter" data-level="6.2" data-path="statistical-inference.html"><a href="statistical-inference.html#probability-models"><i class="fa fa-check"></i><b>6.2</b> Probability models</a></li>
<li class="chapter" data-level="6.3" data-path="statistical-inference.html"><a href="statistical-inference.html#fitting-models"><i class="fa fa-check"></i><b>6.3</b> Fitting models</a></li>
<li class="chapter" data-level="6.4" data-path="statistical-inference.html"><a href="statistical-inference.html#uncertainty-in-estimation"><i class="fa fa-check"></i><b>6.4</b> Uncertainty in estimation</a></li>
<li class="chapter" data-level="6.5" data-path="statistical-inference.html"><a href="statistical-inference.html#good"><i class="fa fa-check"></i><b>6.5</b> What makes an estimator good?</a></li>
<li class="chapter" data-level="6.6" data-path="statistical-inference.html"><a href="statistical-inference.html#assessing-goodness-of-fit"><i class="fa fa-check"></i><b>6.6</b> Assessing goodness-of-fit</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="contingency.html"><a href="contingency.html"><i class="fa fa-check"></i><b>7</b> Contingency tables</a><ul>
<li class="chapter" data-level="7.1" data-path="contingency.html"><a href="contingency.html#way2"><i class="fa fa-check"></i><b>7.1</b> 2-way contingency tables</a><ul>
<li class="chapter" data-level="7.1.1" data-path="contingency.html"><a href="contingency.html#comparing-probabilities"><i class="fa fa-check"></i><b>7.1.1</b> Comparing probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="contingency.html"><a href="contingency.html#way3"><i class="fa fa-check"></i><b>7.2</b> 3-way contingency tables</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="linreg.html"><a href="linreg.html"><i class="fa fa-check"></i><b>8</b> Linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="linreg.html"><a href="linreg.html#simple-linear-regression"><i class="fa fa-check"></i><b>8.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="8.2" data-path="linreg.html"><a href="linreg.html#looking-at-scatter-plots"><i class="fa fa-check"></i><b>8.2</b> Looking at Scatter plots</a></li>
<li class="chapter" data-level="8.3" data-path="linreg.html"><a href="linreg.html#model-checking"><i class="fa fa-check"></i><b>8.3</b> Model checking</a><ul>
<li class="chapter" data-level="8.3.1" data-path="linreg.html"><a href="linreg.html#outliers"><i class="fa fa-check"></i><b>8.3.1</b> Outliers and influential observations</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="linreg.html"><a href="linreg.html#linregtrans"><i class="fa fa-check"></i><b>8.4</b> Use of transformations</a></li>
<li class="chapter" data-level="8.5" data-path="linreg.html"><a href="linreg.html#over-fitting"><i class="fa fa-check"></i><b>8.5</b> Over-fitting</a></li>
<li class="chapter" data-level="8.6" data-path="linreg.html"><a href="linreg.html#other-aspects-of-regression"><i class="fa fa-check"></i><b>8.6</b> Other aspects of regression</a></li>
<li class="chapter" data-level="8.7" data-path="linreg.html"><a href="linreg.html#uncertainty-in-parameter-estimates"><i class="fa fa-check"></i><b>8.7</b> Uncertainty in parameter estimates</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="correlationchapter.html"><a href="correlationchapter.html"><i class="fa fa-check"></i><b>9</b> Correlation</a><ul>
<li class="chapter" data-level="9.1" data-path="correlationchapter.html"><a href="correlationchapter.html#correlation-a-measure-of-linear-association"><i class="fa fa-check"></i><b>9.1</b> Correlation: a measure of linear association</a></li>
<li class="chapter" data-level="9.2" data-path="correlationchapter.html"><a href="correlationchapter.html#covariance-and-correlation"><i class="fa fa-check"></i><b>9.2</b> Covariance and correlation</a></li>
<li class="chapter" data-level="9.3" data-path="correlationchapter.html"><a href="correlationchapter.html#use-and-misuse-of-correlation"><i class="fa fa-check"></i><b>9.3</b> Use and misuse of correlation</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="a-general-strategy-for-statistical-modelling.html"><a href="a-general-strategy-for-statistical-modelling.html"><i class="fa fa-check"></i><b>10</b> A general strategy for statistical modelling</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT0002 Introduction to Probability and Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rvs" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Random variables</h1>
<p><strong>Example</strong>. We return to the space shuttle example.</p>
<p>Consider what happens to the O-rings on a particular test flight, at a particular temperature. A given O-ring either is damaged (shows signs of thermal distress) or it is not damaged. Let <span class="math inline">\(D\)</span> denote the event that an O-ring is damaged and <span class="math inline">\(\bar{D}\)</span> the event that it is not damaged. If we consider all 6 O-rings, there are many possible outcomes in the sample space, <span class="math inline">\(2^6=64\)</span>, in fact:
<span class="math display">\[ S= \{DDDDDD\}, \{DDDDD\bar{D}\}, \ldots, \{D\bar{D}\bar{D}\bar{D}\bar{D}\bar{D}\}, 
\{\bar{D}\bar{D}\bar{D}\bar{D}\bar{D}\bar{D}\}. \]</span>
Suppose that we are not interested in which particular O-rings were damaged, just the total number <span class="math inline">\(N\)</span> of damaged O-rings. The possible values for <span class="math inline">\(N\)</span> are 0,1,2,3,4,5,6.</p>
<p>Each outcome in <span class="math inline">\(S\)</span> gives a value for <span class="math inline">\(N\)</span> in {0,1,2,3,4,5,6}:</p>
<p><span class="math inline">\(\{DDDDDD\}\)</span> gives <span class="math inline">\(N=6\)</span>,</p>
<p><span class="math inline">\(\{DDDDD\bar{D}\}\)</span> gives <span class="math inline">\(N=5\)</span>,</p>
<p><span class="math inline">\(\{DDDD\bar{D}D\}\)</span> gives <span class="math inline">\(N=5\)</span>,</p>
<p><span class="math inline">\(\vdots\)</span></p>
<p><span class="math inline">\(\{\bar{D}\bar{D}\bar{D}\bar{D}\bar{D}\bar{D}\}\)</span> gives <span class="math inline">\(N=0\)</span>.</p>
<p>By defining <span class="math inline">\(N\)</span> to be the total number of damaged O-rings, we have moved from considering outcomes to considering a variable with a numerical value. <span class="math inline">\(N\)</span> is a real-valued function on the sample space <span class="math inline">\(S\)</span>, that is, <span class="math inline">\(N\)</span> maps each outcome in <span class="math inline">\(S\)</span> to a real number. <span class="math inline">\(N\)</span> is a rule that assigns a real number to every outcome <span class="math inline">\(s\)</span> in <span class="math inline">\(S\)</span>. Since the outcomes in <span class="math inline">\(S\)</span> are random the variable <span class="math inline">\(N\)</span> is also random, and we can assign probabilities to its possible values, that is, <span class="math inline">\(P(N=0), P(N=1)\)</span> and so on.</p>
<p><span class="math inline">\(N\)</span> is a <strong>random variable</strong>. In fact, if we assume that O-rings are damaged independently of each other and each O-ring has the same probability <span class="math inline">\(p\)</span> of being damaged, <span class="math inline">\(N\)</span> is a random variable with a special name. It is a binomial random variable with parameters 6 and <span class="math inline">\(p\)</span>. We will consider binomial random variables in more detail in Section <a href="simple.html#binomial">5.2</a>.</p>
<p><strong>Notation</strong>. We denote random variables by upper case letters, for example, <span class="math inline">\(N, X, Y, Z\)</span>. Once we have observed the value of a random variable it is no longer random: it is equal to a particular value. To make this clear we denote sample values of r.v.s. by lower case letters, for example, <span class="math inline">\(n, x, y, z\)</span> and write <span class="math inline">\(N=n, X=x\)</span> and so on. Thus, <span class="math inline">\(P(X=x)\)</span> is the probability that the random variable <span class="math inline">\(X\)</span> has the value <span class="math inline">\(x\)</span>.</p>
<div id="discrete" class="section level2">
<h2><span class="header-section-number">4.1</span> Discrete random variables</h2>
<p><strong>Definition</strong>. A discrete random variable is a random variable that can take only a finite, or countably infinite, number of values.</p>
<p>An example of a countably infinite set of values is {0,1,2,3,}. The random variable <span class="math inline">\(N\)</span> in the space shuttle example takes a finite number of values: 0,1,2,3,4,5,6. Therefore <span class="math inline">\(N\)</span> is a discrete random variable.</p>
<p><strong>Definition</strong>. Let <span class="math inline">\(X\)</span> be a discrete random variable. The <strong>probability mass function (p.m.f.)</strong> <span class="math inline">\(p_X(x)\)</span>, or simply <span class="math inline">\(p(x)\)</span>, of <span class="math inline">\(X\)</span> is
<span class="math display">\[ p_X(x) = P(X=x), \qquad \mbox{for $x$ in the support of $X$}.  \]</span></p>
<p>The p.m.f. of <span class="math inline">\(X\)</span> tells us the probability with which <span class="math inline">\(X\)</span> takes any particular value <span class="math inline">\(x\)</span>. The <strong>support</strong> of <span class="math inline">\(X\)</span> is the set of values that it is possible for <span class="math inline">\(X\)</span> to take. It is very important to write this down every time you write down a p.m.f.. A discrete random variable is completely specified by its probability mass function.</p>
<p><strong>Properties of p.m.f.s</strong></p>
<p>Let <span class="math inline">\(X\)</span> take values <span class="math inline">\(x_1, x_2,\ldots.\)</span> Then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(p_X(x_i) \geq 0\)</span>, for all <span class="math inline">\(i\)</span>,</li>
<li><span class="math inline">\(\displaystyle\sum_i p_X(x_i) = 1\)</span>.</li>
</ol>
<p>Note: 1. is true because the <span class="math inline">\(p_X(x_i)\)</span>s are probabilities; 2. is true because summing over the <span class="math inline">\(x_i\)</span>s is equivalent to summing over the sample space of outcomes.</p>
<p><strong>Definition</strong>. The cumulative distribution function (c.d.f.) of a random variable <span class="math inline">\(X\)</span> is
<span class="math display">\[ F_X(x) = P(X \leq x), \qquad \mbox{for} -\infty &lt; x &lt; \infty. \]</span></p>
<p><strong>Relationship between the c.d.f. and p.m.f. of a discrete random variable</strong>. For a discrete random variable:
<span class="math display">\[ F_X(x) = P(X \leq x) = \sum_{x_i \leq x} P(X = x_i). \]</span>
Therefore, assuming for the moment that the random variable takes only integer values,
<span class="math display">\[ P(X=x) = P(X \leq x) - P(X \leq x-1) = F_X(x) - F_X(x-1) \]</span>
for any integer <span class="math inline">\(x\)</span></p>
</div>
<div id="continuous" class="section level2">
<h2><span class="header-section-number">4.2</span> Continuous random variables</h2>
<p><strong>Example</strong>. We return to the Oxford birth times example.</p>
<p>The top plot in Figure <a href="rvs.html#fig:oxcontvar">4.1</a> shows a histogram of the 95 birth times. The variable of interest in this example is a time. Time is a continuous variable: in principle, the times in this dataset could take any positive real value, uncountably many values. In practice, these times have been recorded discretely, in units of 1/10 of an hour or 1/4 of an hour.</p>
<div class="figure" style="text-align: center"><span id="fig:oxcontvar"></span>
<img src="images/ox_cont_var.png" alt="Top: histogram of the Oxford birth durations. Second from top: histogram of 1,000 values simulated from a distribution fitted to the data. Second from bottom: similarly for 10,000 simulated values. Bottom: p.d.f. of the distribution fitted to the Oxford birth times data." width="80%" />
<p class="caption">
Figure 4.1: Top: histogram of the Oxford birth durations. Second from top: histogram of 1,000 values simulated from a distribution fitted to the data. Second from bottom: similarly for 10,000 simulated values. Bottom: p.d.f. of the distribution fitted to the Oxford birth times data.
</p>
</div>
<p>Suppose that we continue to collect data on birth duration from this hospital, and, as new observations arrive, we add them to the top histogram in Figure <a href="rvs.html#fig:oxcontvar">4.1</a>. We imagine that the times are recorded continuously. As the number of observations <span class="math inline">\(n\)</span> increases we decrease the bin width of the histogram. As <span class="math inline">\(n\)</span> increases to infinity the bin width shrinks to zero and the histogram tends to a smooth continuous curve.</p>
<p>This is shown in the bottom 3 plots in Figure <a href="rvs.html#fig:oxcontvar">4.1</a>. The extra data are not real. They are data I have simulated, using a computer, to have a distribution with a similar shape to the histogram of the real data.</p>
<p>Let <span class="math inline">\(T\)</span> denote the time, in hours, that a woman arriving at the hospital takes to give birth. The smooth continuous curve at the bottom of Figure <a href="rvs.html#fig:oxcontvar">4.1</a> is called the <strong>probability density function (p.d.f.)</strong> <span class="math inline">\(f_T(t)\)</span> of the random variable <span class="math inline">\(T\)</span>. Since the total area of the rectangles in a histogram is equal to 1, the area <span class="math inline">\(\int_{-\infty}^{\infty} f_T(t) {\rm ~d}t\)</span> under the p.d.f. <span class="math inline">\(f_T(t)\)</span> is equal to 1.</p>
<p><strong>Definition</strong>. A <strong>probability density function (p.d.f.)</strong> is a function <span class="math inline">\(f_{X}(x)\)</span>, or simply <span class="math inline">\(f(x)\)</span>, such that</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(f_X(x) \geq 0\)</span>, for <span class="math inline">\(-\infty &lt; x &lt; \infty\)</span>;</li>
<li><span class="math inline">\(\displaystyle\int_{-\infty}^{\infty} f_X(x) {\rm ~d}x = 1\)</span>.</li>
</ol>
<p>Therefore, p.d.f.s are always non-negative and integrate to 1. The support of a continuous random variable is the set of values for which the p.d.f. is positive. Suppose that we wish to find <span class="math inline">\(P(4 &lt; T \leq 12)\)</span>. To find the proportion of times between 4 and 12 using a histogram, we sum the areas of all bins between 4 and 12, that is, we find the area shaded in the histogram in Figure <a href="rvs.html#fig:oxshady">4.2</a>. To do this using the p.d.f. we do effectively the same thing: we find the area under the p.d.f. <span class="math inline">\(f_T(t)\)</span> between 4 and 12. Since <span class="math inline">\(f_T(t)\)</span> is a smooth continuous curve, (that is, the bin widths are zero) we integrate <span class="math inline">\(f_T(t)\)</span> between 4 and 12.</p>
<div class="figure" style="text-align: center"><span id="fig:oxshady"></span>
<img src="images/ox_shady.png" alt="Top: histogram of the Oxford birth durations. Bottom: p.d.f. of the distribution fitted to the Oxford birth duration data." width="80%" />
<p class="caption">
Figure 4.2: Top: histogram of the Oxford birth durations. Bottom: p.d.f. of the distribution fitted to the Oxford birth duration data.
</p>
</div>
<p>Therefore
<span class="math display">\[ P(4 &lt; T \leq 12) = \displaystyle\int_4^{12} f_T(t) {\rm ~d}t = F_T(12)-F_T(4). \]</span></p>
<p>More generally,
<span class="math display">\[ P(a &lt; T \leq b) = \displaystyle\int_a^b f_T(t) {\rm ~d}t = F_T(b)-F_T(a). \]</span></p>
<p><strong>Definition</strong>. A random variable <span class="math inline">\(X\)</span> is a <strong>continuous random variable</strong> if there exists a p.d.f. <span class="math inline">\(f_X(x)\)</span> such that
<span class="math display">\[
P(a &lt; X \leq b) = \int_{a}^{b} f_X(x) {\rm ~d}x,
\]</span>
for all <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that <span class="math inline">\(a &lt; b\)</span>.</p>
<p>Figure <a href="rvs.html#fig:pdfshady">4.3</a> illustrates the properties of a p.d.f..</p>
<div class="figure" style="text-align: center"><span id="fig:pdfshady"></span>
<img src="images/pdf_shady.png" alt="Properties of a p.d.f.. The areas that correspond to the probability that a random variable takes a value in a given interval are shaded." width="80%" />
<p class="caption">
Figure 4.3: Properties of a p.d.f.. The areas that correspond to the probability that a random variable takes a value in a given interval are shaded.
</p>
</div>
<p>Notes</p>
<ul>
<li>It is very important to appreciate that <span class="math inline">\(f_X(x)\)</span> is <strong>not</strong> a probability: it does <strong>not</strong> give <span class="math inline">\(P(X=x)\)</span>. In fact <span class="math inline">\(P(X=x)=0\)</span>: the probability that a continuous random variable <span class="math inline">\(X\)</span> takes the value <span class="math inline">\(x\)</span> is zero.</li>
<li>Indeed, it is possible for a p.d.f. to be greater than 1. Consider a continuous random variable <span class="math inline">\(X\)</span> with p.d.f.
<span class="math display">\[ f_X(x) = \left\{ \begin{array}{ll} 2\,(1-x) &amp; \,0 \leq x \leq 1, \\ 0 &amp; \,\mbox{otherwise}.\end{array}\right. \]</span>
For this random variable <span class="math inline">\(f_X(x)&gt;1\)</span> for any <span class="math inline">\(x \in [0, 1/2)\)</span> .</li>
<li>Since <span class="math inline">\(P(X=x)=0\)</span>
<span class="math display">\[ P(a &lt; X \leq b) = P(a \leq X \leq b) = P(a \leq X &lt; b) = P(a &lt; X &lt; b). \]</span></li>
<li><span class="math inline">\(f_X(x)\)</span> is a probability <strong>density</strong>. The probability that <span class="math inline">\(X\)</span> lies in a very small interval of length <span class="math inline">\(\delta\)</span> near <span class="math inline">\(x\)</span> is approximately <span class="math inline">\(f_X(x) \delta\)</span>. For the p.d.f. at the bottom of figure <a href="rvs.html#fig:oxcontvar">4.1</a>, <span class="math inline">\(f_T(6) &gt; f_T(12)\)</span>, indicating that a randomly chosen woman is more likely to spend approximately 6 hours giving birth than approximately
12 hours.</li>
</ul>
<p><strong>Relationship between the c.d.f. and p.d.f. of a continuous random variable</strong>. For a continuous random variable
<span class="math display">\[ F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(u) {\rm ~d}u. \]</span>
Therefore,
<span class="math display">\[ f_X(x) = \frac{{\rm d}}{{\rm d}x} F_X(x). \]</span></p>
</div>
<div id="expectation" class="section level2">
<h2><span class="header-section-number">4.3</span> Expectation</h2>
<p>The expectation of a random variable is a measure of the location of its distribution.</p>
<div id="expectation-of-a-discrete-random-variable" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Expectation of a discrete random variable</h3>
<p><strong>Example</strong>. We return to the space shuttle example.</p>
<p>Again we consider test flights conducted at a particular temperature, say 53<span class="math inline">\(^\circ\)</span>F. Suppose that NASA are able to conduct a very large number <span class="math inline">\(n\)</span> of test flights at 53<span class="math inline">\(^\circ\)</span>F, producing a sample <span class="math inline">\(x_1,\ldots,x_n\)</span> of numbers of damaged O-rings.</p>
<p>Let <span class="math inline">\(n(x)\)</span> be the number of test flights on which <span class="math inline">\(x\)</span> of the 6 O-rings were damaged. We can write the sample mean <span class="math inline">\(\bar{x}\)</span> of <span class="math inline">\(x_1,\ldots,x_n\)</span> as
<span class="math display">\[\begin{eqnarray*}
\bar{x} &amp;=&amp; \frac{0 \times n(0) + 1 \times n(1) + \cdots + 6 \times n(6)}{n}, \\
&amp;=&amp; \sum_{x=0}^6 x\,\frac{n(x)}{n}.
\end{eqnarray*}\]</span>
As the sample size <span class="math inline">\(n\)</span> increases to infinity, the sample proportion <span class="math inline">\(n(x)/n\)</span> tends to <span class="math inline">\(P(X=x)\)</span>, for <span class="math inline">\(x=0,1,\ldots,6\)</span>. Therefore, in the limit as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(\bar{x}\)</span> tends to
<span class="math display" id="eq:shuttlemean">\[\begin{eqnarray}
\sum_{x=0}^6 x\,P(X=x). 
\tag{4.1}
\end{eqnarray}\]</span>
This is known as the mean of the probability distribution of <span class="math inline">\(X\)</span>. It is a measure of the location of the distribution.</p>
<p>The quantity in equation <a href="rvs.html#eq:shuttlemean">(4.1)</a> is the value of the sample mean <span class="math inline">\(\bar{x}\)</span> that we would expect to get from a very large sample. Therefore it is often called the <strong>expectation</strong> or <strong>expected value</strong> of the random variable <span class="math inline">\(X\)</span> and it is denoted <span class="math inline">\(\mathrm{E}(X)\)</span>.</p>
<p><strong>Definition</strong>. The <strong>expectation</strong> (or <strong>expected value</strong> or <strong>mean</strong>) <span class="math inline">\(\mathrm{E}(X)\)</span> of a discrete random variable <span class="math inline">\(X\)</span> is given by
<span class="math display" id="eq:discmean">\[\begin{eqnarray}
\mathrm{E}(X) &amp;=&amp; \sum_x x\,P(X=x). 
\tag{4.2}
\end{eqnarray}\]</span>
This is a weighted average of the values that <span class="math inline">\(X\)</span> can take, each value being weighted by <span class="math inline">\(P(X=x)\)</span>.</p>
<p>Note:</p>
<ul>
<li>We often write <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\mu_X\)</span> for <span class="math inline">\(\mathrm{E}(X)\)</span>.</li>
<li>Units: the units of <span class="math inline">\(\mathrm{E}(X)\)</span> are the same as those of <span class="math inline">\(X\)</span>. For example, if <span class="math inline">\(X\)</span> is measured in hours then <span class="math inline">\(\mathrm{E}(X)\)</span> is measured in hours.</li>
<li><span class="math inline">\(\mathrm{E}(X)\)</span> exists only if <span class="math inline">\(\sum_x |x|\,P(X=x) &lt; \infty\)</span>. If the number of values <span class="math inline">\(X\)</span> can take is finite then <span class="math inline">\(\mathrm{E}(X)\)</span> will always exist.</li>
</ul>
</div>
<div id="expectation-of-a-continuous-random-variable" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Expectation of a continuous random variable</h3>
<p>We can define the expectation of a continuous random variable in a similar way to a discrete random variable, replacing summation with integration.</p>
<p><strong>Definition</strong>.
The expectation <span class="math inline">\(\mathrm{E}(X)\)</span> of a continuous random variable <span class="math inline">\(X\)</span> is given by
<span class="math display" id="eq:contmean">\[\begin{eqnarray}
\mathrm{E}(X) = \int_{-\infty}^{\infty} x\,f_X(x) {\rm ~d}x. 
\tag{4.3}
\end{eqnarray}\]</span>
Note:</p>
<ul>
<li>Like the discrete case, this is a weighted average of the values that <span class="math inline">\(X\)</span> can take, but now each value is weighted by the
p.d.f. <span class="math inline">\(f_X(x)\)</span>.</li>
<li>The range of integration in equation <a href="rvs.html#eq:contmean">(4.3)</a> is over the whole real line but, in practice, integration will be over the range of possible values of <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(\mathrm{E}(X)\)</span> exists only if <span class="math inline">\(\int_{-\infty}^{\infty} |x|\,f_X(x) {\rm ~d}x &lt; \infty\)</span>.</li>
</ul>
</div>
<div id="properties-of-mathrmex" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Properties of <span class="math inline">\(\mathrm{E}(X)\)</span></h3>
<p>If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants then
<span class="math display">\[ \mathrm{E}(a\,X+b) = a\,\mathrm{E}(X)+b. \]</span>
This makes sense. If we multiply all observations by <span class="math inline">\(a\)</span> their mean will also be multiplied by <span class="math inline">\(a\)</span>. If we add <span class="math inline">\(b\)</span> to all observations their mean will be increased by <span class="math inline">\(b\)</span>, that is, the distribution of <span class="math inline">\(X\)</span> shifts up by <span class="math inline">\(b\)</span>.</p>
<ul>
<li>If <span class="math inline">\(X \geq 0\)</span> then <span class="math inline">\(\mathrm{E}(X) \geq 0\)</span>.</li>
<li>If <span class="math inline">\(X\)</span> is a constant <span class="math inline">\(c\)</span>, that is, <span class="math inline">\(P(X=c)=1\)</span> then <span class="math inline">\(\mathrm{E}(X)=c\)</span>.</li>
<li>It can be shown that
<span class="math display">\[ \mathrm{E}(X_1 + X_2 + \cdots + X_n) = \mathrm{E}(X_1) + \mathrm{E}(X_2) + \cdots + \mathrm{E}(X_n). \]</span></li>
</ul>
</div>
<div id="the-expectation-of-gx" class="section level3">
<h3><span class="header-section-number">4.3.4</span> The expectation of <span class="math inline">\(g(X)\)</span></h3>
<p>Suppose that <span class="math inline">\(Y=g(X)\)</span> is a function of of <span class="math inline">\(X\)</span>, such as <span class="math inline">\(aX+b\)</span>, <span class="math inline">\(X^2\)</span> or <span class="math inline">\(\log X\)</span>. Then <span class="math inline">\(Y\)</span> is also a random variable. If we find the p.m.f (if <span class="math inline">\(Y\)</span> is discrete) or p.d.f. (if <span class="math inline">\(Y\)</span> is continuous) of <span class="math inline">\(Y\)</span> then we can find the the expectation of <span class="math inline">\(Y\)</span> using equation <a href="rvs.html#eq:discmean">(4.2)</a> or <a href="rvs.html#eq:contmean">(4.3)</a> as appropriate.</p>
<p><span class="math display" id="eq:expfn">\[\begin{equation}
\mathrm{E}(Y) = \mathrm{E}[g(X)] =
\begin{cases} 
\displaystyle\sum_x g(x)\,P(X=x) &amp; \text{if } X \text{ is discrete}, \\
\int_{-\infty}^{\infty} g(x)\,f_X(x) {\rm ~d}x &amp; \text{if } X \text{ is continuous}.
\end{cases}
\tag{4.4}
\end{equation}\]</span></p>
<p>Note, it is usually the case that
<span class="math display">\[ \mathrm{E}[g(X)] \neq g[\mathrm{E}(X)] \]</span>
although there are exceptions.</p>
</div>
</div>
<div id="variance" class="section level2">
<h2><span class="header-section-number">4.4</span> Variance</h2>
<p>The variance of a random variable is a measure of the spread of its distribution.</p>
<div id="variance-of-a-discrete-random-variable" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Variance of a discrete random variable</h3>
<p><strong>Example</strong>. We return the space shuttle example.</p>
<p>As before we let <span class="math inline">\(n(x)\)</span> be the number of test flights on which <span class="math inline">\(x\)</span> of the 6 O-rings were damaged. We saw in Section <a href="descriptive-statistics.html#meanstdev">2.3.2</a> that a measure of the spread of a sample <span class="math inline">\(x_1,\ldots,x_n\)</span> is the sample variance <span class="math inline">\(s_X^2\)</span> which, in this example, can be written as</p>
<p><span class="math display">\[\begin{eqnarray*}
s_X^2 &amp;=&amp; \frac{1}{n-1}\,\left\{
(0-\bar{x})^2\,n(0)+(1-\bar{x})^2\,n(1)+\cdots+(6-\bar{x})^2\,n(6) \right\},
\\
      &amp;=&amp; \sum_{x=0}^6 (x-\bar{x})^2\,\frac{n(x)}{n-1}. 
\end{eqnarray*}\]</span>
As the sample size <span class="math inline">\(n\)</span> increases to infinity, <span class="math inline">\(\frac{n(x)}{n-1}\)</span> tends to <span class="math inline">\(P(X=x)\)</span>, for <span class="math inline">\(x=0,1,\ldots,6\)</span> and <span class="math inline">\(\bar{x}\)</span> tends to <span class="math inline">\(\mu\)</span>=<span class="math inline">\(\mathrm{E}(X)\)</span>.</p>
<p>Therefore, as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(s_X^2\)</span> tends to</p>
<p><span class="math display" id="eq:shuttlevar">\[\begin{equation}
\sum_{x=0}^6 (x-\mu)^2\,P(X=x). 
\tag{4.5}
\end{equation}\]</span></p>
<p>This is known as the variance of the probability distribution of <span class="math inline">\(X\)</span>. It is a measure of the spread of the distribution. The quantity in equation <a href="rvs.html#eq:shuttlevar">(4.5)</a> is the value of the sample variance <span class="math inline">\(s_X^2\)</span> that we would expect to get from a very large sample.</p>
<p><strong>Definition</strong>. The variance <span class="math inline">\(\mathrm{var}(X)\)</span> of a discrete random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mathrm{E}(X)=\mu\)</span> is given by</p>
<p><span class="math display" id="eq:varidisc">\[\begin{equation}
\mathrm{var}(X) = \sum_x\,(x-\mu)^2\,P(X=x). 
\tag{4.6}
\end{equation}\]</span></p>
<p>This is a weighted average of the squared differences between the values that <span class="math inline">\(X\)</span> can take and its mean <span class="math inline">\(\mu\)</span>, each value being weighted by <span class="math inline">\(P(X=x)\)</span>.</p>
<p>Note:</p>
<ul>
<li><span class="math inline">\(\mathrm{var}(X)\)</span> exists only if <span class="math inline">\(\mu\)</span> exists and <span class="math inline">\(\sum_x (x-\mu)^2\,P(X=x) &lt; \infty\)</span>. If the number of values <span class="math inline">\(X\)</span> can take is finite then <span class="math inline">\(\mathrm{var}(X)\)</span> will always exist.</li>
<li>We often write <span class="math inline">\(\sigma^2\)</span> or <span class="math inline">\(\sigma_X^2\)</span> for <span class="math inline">\(\mathrm{var}(X)\)</span>.</li>
</ul>
</div>
<div id="variance-of-a-continuous-random-variable" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Variance of a continuous random variable</h3>
<p>We can define the variance of a continuous random variable in a similar way to a discrete random variable, replacing summation with integration.</p>
<p><strong>Definition</strong>. The variance <span class="math inline">\(\mathrm{var}(X)\)</span> of a continuous random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mathrm{E}(X)=\mu\)</span> is given by</p>
<p><span class="math display" id="eq:varicont">\[\begin{equation}
\mathrm{var}(X) = \int_{-\infty}^{\infty} (x-\mu)^2 f_X(x) {\rm ~d}x. 
\tag{4.7}
\end{equation}\]</span></p>
<p>Note: <span class="math inline">\(\mathrm{var}(X)\)</span> exists only if <span class="math inline">\(\mu\)</span> exists and <span class="math inline">\(\int_{-\infty}^{\infty} (x-\mu)^2\,f_X(x) {\rm ~d}x &lt; \infty\)</span>.</p>
</div>
<div id="variance-and-standard-deviation" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Variance and standard deviation</h3>
<p><strong>Definition</strong>. Let <span class="math inline">\(X\)</span> be a random variable with <span class="math inline">\(\mathrm{E}(X)=\mu\)</span>. The variance <span class="math inline">\(\mathrm{var}(X)\)</span> is given by
<span class="math display">\[ \mathrm{var}(X) = \mathrm{E}\left[(X-\mu)^2\right]. \]</span>
This follows from equations <a href="rvs.html#eq:varidisc">(4.6)</a> and <a href="rvs.html#eq:varicont">(4.7)</a> and the expression in equation <a href="rvs.html#eq:expfn">(4.4)</a> for the expectation of a function <span class="math inline">\(g(X)\)</span> of a random variable <span class="math inline">\(X\)</span>.</p>
<p>There is an alternative way to calculate <span class="math inline">\(\mathrm{var}(X)\)</span>:
<span class="math display">\[ \mathrm{var}(X) = \mathrm{E}\left(X^2\right) - [\mathrm{E}(X)]^2. \]</span></p>
<p><strong>Exercise</strong>. Prove this.</p>
<p><strong>Definition</strong>. The standard deviation sd<span class="math inline">\((X)\)</span> of <span class="math inline">\(X\)</span> is given by sd(<span class="math inline">\(X\)</span>)=<span class="math inline">\(+\sqrt{\mathrm{var}(X)}\)</span>.</p>
<p>Notes on <span class="math inline">\(\mathrm{var}(X)\)</span>:</p>
<ul>
<li><span class="math inline">\(\mathrm{var}(X) \geq 0\)</span>. A variance cannot be negative.<br />
</li>
<li>Units: the units of <span class="math inline">\(\mathrm{var}(X)\)</span> are the square of those of <span class="math inline">\(X\)</span>. For example, if <span class="math inline">\(X\)</span> is measured in hours then <span class="math inline">\(\mathrm{var}(X)\)</span> is measured in hours<span class="math inline">\(^2\)</span> (and sd(<span class="math inline">\(X\)</span>) is measured in hours).</li>
</ul>
</div>
<div id="properties-of-mathrmvarx" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Properties of <span class="math inline">\(\mathrm{var}(X)\)</span></h3>
<ul>
<li>If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants then
<span class="math display">\[ \mathrm{var}(a\,X+b) = a^2\,\mathrm{var}(X). \]</span>
This makes sense. If we multiply all observations by <span class="math inline">\(a\)</span> their variance, which is measured square units, will be multiplied by <span class="math inline">\(a^2\)</span>. If we add <span class="math inline">\(b\)</span> to all observations their variance will be unchanged because the distribution simply shifts up by <span class="math inline">\(b\)</span> and its spread is unaffected.</li>
<li>If <span class="math inline">\(X\)</span> is a constant <span class="math inline">\(c\)</span>, that is, <span class="math inline">\(P(X=c)=1\)</span> then <span class="math inline">\(\mathrm{var}(X)=0\)</span>: the distribution of <span class="math inline">\(X\)</span> has zero spread.</li>
<li>It can also be shown that <strong>if the random variables <span class="math inline">\(X_1, X_2, \ldots X_n\)</span> are independent</strong> then</li>
</ul>
<p><span class="math display" id="eq:varsum">\[\begin{equation}
\mathrm{var}(X_1 + X_2 + \cdots + X_n) = \mathrm{var}(X_1) + \mathrm{var}(X_2) + \cdots + \mathrm{var}(X_n). 
\tag{4.8}
\end{equation}\]</span></p>
<p><strong>Note</strong>. Independence is sufficient for this result to hold but it is not necessary. Taking <span class="math inline">\(n=2\)</span> as an example, in generality we have
<span class="math display">\[ \mathrm{var}(X_1 + X_2) = \mathrm{var}(X_1) + \mathrm{var}(X_2) + 2\,\mathrm{cov}(X_1,X_2), \]</span>
where <span class="math inline">\({\rm cov}(X_1,X_2)\)</span> is the <strong>covariance</strong> between the random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Covariance is a measure of the strength of <strong>linear</strong> association. If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent (have no association of any kind) then <span class="math inline">\(\mathrm{cov}(X_1,X_2)=0\)</span>, because they have no linear association. However, it is possible for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> to be dependent but <span class="math inline">\(\mathrm{cov}(X_1,X_2)=0\)</span>, because, although they have some kind of association, they have no <strong>linear</strong> association. Thus, independence is a stronger requirement than zero covariance.</p>
<p>Returing to general <span class="math inline">\(n\)</span> we have
<span class="math display">\[ \mathrm{var}(X_1 + X_2 + \cdots + X_n) = \mathrm{var}(X_1) + \mathrm{var}(X_2) + \cdots + \mathrm{var}(X_n) + 2 \mathop{\sum\sum}_{i &lt; j} {\rm cov}(X_i,X_j). \]</span>
If <span class="math inline">\(\mathrm{cov}(X_i,X_j)=0\)</span> for all <span class="math inline">\(i &lt; j\)</span> then equation <a href="rvs.html#eq:varsum">(4.8)</a> holds. We will study covariance, and its standardised form <strong>correlation</strong>, in Section <a href="correlationchapter.html#correlationchapter">9</a>.</p>
</div>
</div>
<div id="locations" class="section level2">
<h2><span class="header-section-number">4.5</span> Other measures of location</h2>
<div id="the-median-of-a-random-variable" class="section level3">
<h3><span class="header-section-number">4.5.1</span> The median of a random variable</h3>
<p>Recall that the sample median of a set of observations is the middle observation when the observations are arranged in order of size. We define the median of a random variable <span class="math inline">\(X\)</span> as the value, median(<span class="math inline">\(X\)</span>), such that</p>
<p><span class="math display">\[ P(X &lt; \mathrm{median}(X)) \leq \frac12 \leq P(X \leq \mathrm{median}(X)). \]</span></p>
<p>In other words, <span class="math inline">\(\mathrm{median}(X)\)</span> is the value where a plot of the c.d.f. <span class="math inline">\(F_X(x)=P(X \leq x)\)</span> crosses <span class="math inline">\(1/2\)</span>.</p>
<p>For a continuous random variable <span class="math inline">\(X\)</span> we have
<span class="math display">\[  F_X(\mathrm{median}(X)) = P(X \leq \mathrm{median}(X)) =\frac12. \]</span>
and the median will divide the distribution into two parts, each with probability 1/2:
<span class="math display">\[ P(X &lt; \mathrm{median}(X)) = P(X &gt; \mathrm{median}(X)) = \frac12. \]</span></p>
<p>This will not necessarily hold for a discrete distribution. For example, suppose that
<span class="math display">\[ P(X=0)=\frac16, \qquad  P(X=1)=\frac12, \qquad P(X=2)=\frac13. \]</span></p>
<p>Then
<span class="math display">\[\begin{eqnarray*}
F_X(x) = P(X \leq x) = \left\{\begin{array}{ll}
0 &amp; \mbox{for } x &lt;0, \\
\frac16 &amp; \mbox{for } 0 \leq x &lt; 1, \\
\frac23 &amp; \mbox{for } 1 \leq x &lt; 2, \\
1 &amp; \mbox{for } x \geq 2, 
\end{array}\right.
\end{eqnarray*}\]</span>
Therefore, <span class="math inline">\(\mathrm{median}(X) = 1\)</span>. However, <span class="math inline">\(P(X&lt;1)=\frac16\)</span> and <span class="math inline">\(P(X&gt;1)=\frac13\)</span>.</p>
</div>
<div id="the-mode-of-a-random-variable" class="section level3">
<h3><span class="header-section-number">4.5.2</span> The mode of a random variable</h3>
<p>Recall that the sample mode of categorical or discrete data is the value (or values) which occurs most often. We define the mode, mode(<span class="math inline">\(X\)</span>), of a random variable as follows.</p>
<p>For a discrete random variable <span class="math inline">\(X\)</span>, the mode is the value which has the highest probability of occurring: <span class="math inline">\(P(X=\mathrm{mode}(X))\)</span> will be larger than for any other value <span class="math inline">\(X\)</span> can have. In other words, <span class="math inline">\(\mathrm{mode}(X)\)</span> is the value at which the p.m.f. is maximised.</p>
<p>For a continuous random variable <span class="math inline">\(X\)</span>, the mode is the value at which the p.d.f. is maximised. <strong>If the maximum occurs at a turning point of <span class="math inline">\(f_X(x)\)</span></strong> then it can be found by solving the equation
<span class="math display">\[ \frac{{\rm d}}{{\rm d}x} f_X(x)  = 0, \]</span>
and checking that you have indeed found a maximum.</p>
</div>
</div>
<div id="quantiles" class="section level2">
<h2><span class="header-section-number">4.6</span> Quantiles</h2>
<p>To keep things simple we consider a <strong>continuous</strong> random variable <span class="math inline">\(X\)</span>. The <span class="math inline">\(100p\%\)</span> quantile of <span class="math inline">\(X\)</span> is defined to be the value <span class="math inline">\(x_p\)</span> such that
<span class="math display">\[ F_X(x_p)=P(X \leq x_p) = p. \]</span>
Thus, <span class="math inline">\(x_{1/4}\)</span> is the lower quartile of <span class="math inline">\(X\)</span>, <span class="math inline">\(x_{1/2}\)</span> is the median of <span class="math inline">\(X\)</span> and <span class="math inline">\(x_{3/4}\)</span> is the upper quartile of <span class="math inline">\(X\)</span>. The inter-quartile range is <span class="math inline">\(x_{3/4}-x_{1/4}\)</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["stat0002book.pdf", "stat0002book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
